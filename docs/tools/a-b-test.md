---
title: A/B test
lang: en-US
description: Useful frameworks and methodologies for design strategy, research and testing
meta:
  - name: keywords
    content: design, design thinking, UX, user experience, user research, user testing
---

# A/B test

### _Phase:_ üõ†Ô∏è  Problem Solving<br/> _Focus:_ [Test](/tools/#test)

::: tip IN BRIEF
**Time commitment:** A few hours to a day to create the test; time to test depends on number of participants and testing mechanism  
**Difficulty:** Easy  
**Materials needed:** Two (or more) versions of an artifact to be tested, test plan, users, interviewer/notetaker/notetaking tools (if moderated), testing mechanism/platform (if unmoderated)  
**Who should participate:** User experience designers, visual designers, product/project owners  
**Best for:** A quick means of choosing between two similar options when each is at a level of close or moderate detail
:::

## About this tool

A/B testing is used to compare two different versions of a design, and can be used at any stage of the design process once you have a robust enough prototype to get your point across ‚Äî this could be anything from a wireframe or [low-fidelity prototype](low-fidelity-prototype.md) to a slightly different version of an already-live web page.

In this type of test, you create two different prototypes and test each version on a different set of users. This might mean testing something like different header or button copy, a slightly different layout of the same information, rearranging navigational elements, or different positioning of a key piece of text. Then, you design a test plan with tasks intended to test the performance of your variable; for example, if you're testing the placement of navigational elements, ask the users specifically to interact with those navigational elements in a way that measures their ability to find them easily.

Because you're comparing apples to apples and test tasks are usually very simple, A/B testing is a good fit for [unmoderated testing](unmoderated-testing.md). However, if you're interested in gleaning data on _why_ the user did what they did when they executed your task, consider [moderated testing](moderated-testing.md) in which the initial task prompt is followed up with an interview.

### What if you need to test more than one thing?

Testing variations on more than one section/feature of a product (multivariate testing) is possible if your number of variations is small, but it's more complicated to set up than regular A/B testing and the results are more prone to statistical noise. Unless time or budget strongly indicates a need to undergo multivariate testing, it's often easier from both a planning and an interpretation perspective to separate your test plan into several A/B tests.

However, if you want to test more than two versions of the _same_ thing ‚Äî such as three options for a button color ‚Äî an "A/B/C" test is still a relatively easy prospect, given you have a large enough sample size.

## Links and resources

* [Step-by-step guide to A/B testing in prototypes](https://uxplanet.org/an-ultimate-guide-to-a-b-testing-on-pre-live-apps-4bd57679e8cc)
* [Rundown of top online A/B testing tools, circa 2020](https://blog.hubspot.com/marketing/a-b-testing-tools)
* [18F guide to multivariate testing](https://methods.18f.gov/validate/multivariate-testing/)
* [Thoughts on when to combine A/B testing with moderated, interview-based usability testing](https://www.uxmatters.com/mt/archives/2011/07/pairing-up-usability-testing-with-ab-testing.php)
* [A/B testing as a low-risk exercise](https://medium.com/vivareal-ux-chapter/why-ux-designers-should-care-more-about-a-b-testing-7ef88eaee3e9)